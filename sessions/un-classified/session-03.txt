Optimize Data Storage for Reading or Writing

    Writes are cheaper than reads in Cassandra due to its storage engine. 
    Writing data means simply appending something to a so-called commit-log.

    Commit-logs are append-only logs of all mutations local to a Cassandra node 
    and reduce the required I/O to a minimum.

    Reading is more expensive, because it might require checking different 
    disk locations until all the query data is eventually found.

    But this does not mean Cassandra is terrible at reading. Instead, 
    Cassandra's storage engine can be tuned for reading performance or writing performance.


Understanding Compaction

    The foundation for storing data are the so-called SSTables. 
    SSTables are immutable data files Cassandra uses to persist data on disk.

    You can set various strategies for a table that define how data should be merged and compacted. These strategies affect read and write performance:

    1. SizeTieredCompactionStrategy is the default, and is especially performant if you have more writes than reads,

    2. LeveledCompactionStrategy optimizes for reads over writes. This optimization can be costly and needs to be tried out in production carefully
    
    3. TimeWindowCompactionStrategy is for Time-series data

    By default, tables use the SizeTieredCompactionStrategy:

    cqlsh> 
        DESCRIBE TABLE learn_cassandra.users_by_country;

        CREATE TABLE learn_cassandra.users_by_country (
        country text,
        user_email text,
        age smallint,
        first_name text,
        last_name text,
        PRIMARY KEY (country, user_email)
    ) WITH CLUSTERING ORDER BY (user_email ASC)
        AND bloom_filter_fp_chance = 0.01
        AND caching = {'keys': 'ALL', 'rows_per_partition': 'NONE'}
        AND comment = ''
        AND compaction = {'class': 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy', 'max_threshold': '32', 'min_threshold': '4'}
        AND compression = {'chunk_length_in_kb': '64', 'class': 'org.apache.cassandra.io.compress.LZ4Compressor'}
        AND crc_check_chance = 1.0
        AND dclocal_read_repair_chance = 0.1
        AND default_time_to_live = 0
        AND gc_grace_seconds = 864000
        AND max_index_interval = 2048
        AND memtable_flush_period_in_ms = 0
        AND min_index_interval = 128
        AND read_repair_chance = 0.0
        AND speculative_retry = '99PERCENTILE';


    Although you can alter the compaction strategy of an existing table, 
    I would not suggest doing so, because all Cassandra nodes start this migration simultaneously. This will lead to significant performance issues in a production system.

    Instead, define the compaction strategy explicitly during table creation of your new table:

    cqlsh> 
        CREATE TABLE learn_cassandra.users_by_country_with_leveled_compaction (
            country text,
            user_email text,
            first_name text,
            last_name text,
            age smallint,
            PRIMARY KEY ((country), user_email)
        ) WITH
        compaction = { 'class' :  'LeveledCompactionStrategy'  };

    verify your changes:

    cqlsh> 
        DESCRIBE TABLE learn_cassandra.users_by_country_with_leveled_compaction;

    Presorting Data on Cassandra Nodes

        A table always requires a primary key. A primary key consists of 2 parts:

            At least 1 column(s) as partition key and
            Zero or more clustering columns for nesting rows of the data.

        All columns of the partition key together are used to identify partitions. 
        All primary key columns, meaning partition key and clustering columns, identify a specific row within a partition.

        In our users_by_country table, you can define age as another clustering column to sort stored data:

        cqlsh> 
            CREATE TABLE learn_cassandra.users_by_country_sorted_by_age_asc (
                country text,
                user_email text,
                first_name text,
                last_name text,
                age smallint,
                PRIMARY KEY ((country), age, user_email)
            ) WITH CLUSTERING ORDER BY (age ASC);

        Letâ€™s add the same data again:

        cqlsh> 
            INSERT INTO learn_cassandra.users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)
            VALUES('US','john@email.com', 'John','Wick',10);

            INSERT INTO learn_cassandra.users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)
            VALUES('UK', 'peter@email.com', 'Peter','Clark',30);

            INSERT INTO learn_cassandra.users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)
            VALUES('UK', 'bob@email.com', 'Bob','Sandler',20);

            INSERT INTO learn_cassandra.users_by_country_sorted_by_age_asc (country,user_email,first_name,last_name,age)
            VALUES('UK', 'alice@email.com', 'Alice','Brown',40);

        And get the data by country:

        cqlsh> 
            SELECT * FROM learn_cassandra.users_by_country_sorted_by_age_asc WHERE country='UK';

        country | age | user_email       | first_name | last_name
        ---------+-----+------------------+------------+-----------
            UK |  20 | bob@email.com   |        Bob |   Sandler
            UK |  30 | peter@email.com |      Peter |     Clark
            UK |  40 | alice@email.com |      Alice |     Brown

        (3 rows)

    Data Modeling

        cqlsh> 
            CREATE TABLE learn_cassandra.todo_by_user_email (
                user_email text,
                name text,
                creation_date timestamp,
                PRIMARY KEY ((user_email), creation_date)
            ) WITH CLUSTERING ORDER BY (creation_date DESC)
            AND compaction = { 'class' :  'LeveledCompactionStrategy'  };

        cqlsh> 
            CREATE TABLE learn_cassandra.todos_shared_by_target_user_email (
                target_user_email text,
                source_user_email text,
                creation_date timestamp,
                name text,
                PRIMARY KEY ((target_user_email), creation_date)
            ) WITH CLUSTERING ORDER BY (creation_date DESC)
            AND compaction = { 'class' :  'LeveledCompactionStrategy'  };

            CREATE TABLE learn_cassandra.todos_shared_by_source_user_email (
                target_user_email text,
                source_user_email text,
                creation_date timestamp,
                name text,
                PRIMARY KEY ((source_user_email), creation_date)
            ) WITH CLUSTERING ORDER BY (creation_date DESC)
            AND compaction = { 'class' :  'LeveledCompactionStrategy'  };

        
        it's all about defining tables and thinking about what values 
        you want to filter and need to display.

        You need to set a partition key to ensure the data is organised 
        for efficient read and write operations. Also, you need to set 
        clustering columns to ensure uniqueness, sort order, and optional query parameters.

    
    Keep Data in Sync Using BATCH Statements
        
        Due to the duplication, you need to take care to keep data consistent. In Cassandra, you can do that by using BATCH statements that give you an all-at-once guarantee, also called atomicity.

        cqlsh> 

            BEGIN BATCH
            INSERT INTO learn_cassandra.todo_by_user_email (user_email,creation_date,name) VALUES('alice@email.com', toTimestamp(now()), 'My first todo entry')

            INSERT INTO learn_cassandra.todos_shared_by_target_user_email (target_user_email, source_user_email,creation_date,name) VALUES('bob@email.com', 'alice@email.com',toTimestamp(now()), 'My first todo entry')

            INSERT INTO learn_cassandra.todos_shared_by_source_user_email (target_user_email, source_user_email,creation_date,name) VALUES('alice@email.com', 'bob@email.com', toTimestamp(now()), 'My first todo entry')

            APPLY BATCH;

    Use Foreign Keys Instead of Duplicating Data in Cassandra

        Normalizing tables is against a lot of principles in Cassandra. You can reference data by ID, but keep in mind this means you need to join the data yourself. This also means reading and writing data to multiple partitions at once.

        Cassandra is built for scale. If you start normalizing your schema to reduce duplication, then you sacrifice horizontal scalability.

        If you still want to use foreign keys instead of data duplication, you could use a database that sacrifices performance and availability, and gives more consistency guarantees.


    Tombstones

        Cassandra is a multi-node cluster that contains replicated data on different nodes. 
        Therefore, a delete can not simply delete a particular record.

        For a delete operation, a new entry is added to the commit-log like for any other insert and update mutation. 
        These deletes are called tombstones, and they flag a specific value for deletion.

        In Cassandra, you can set a time to live on inserted data. 
        After the time passed, the record will be automatically deleted. 
        When you set a time to live (TTL), a tombstone is created with a date in the future.

        cqlsh>  
          INSERT INTO learn_cassandra.todo_by_user_email (user_email,creation_date,name) VALUES('john@email.com', toTimestamp(now()), 'This entry should be removed soon') USING TTL 60;

        cqlsh>      
            SELECT * FROM learn_cassandra.todo_by_user_email WHERE user_email='john@email.com';

            user_email    | creation_date | name
            ----------------+---------------+--------------------
            john@email.com | 2021-05-30... | This entry should be removed soon

            (1 rows)
        
        cqlsh> 
            SELECT TTL(name) FROM learn_cassandra.todo_by_user_email WHERE user_email='john@email.com';

            ttl(name)
            -----------
                    43

            (1 rows)

        After 60 seconds, the row is gone.

        cqlsh>  
            SELECT * FROM learn_cassandra.todo_by_user_email WHERE user_email='john@email.com';                                  

            user_email | creation_date | todo_uuid | name
            -----------+---------------+-----------+------

            (0 rows)

        Setting a TTL is one of many ways to create and execute tombstones.

        For example, when you insert a null value, a tombstone is created for the given cell. 
        And as mentioned for delete requests, different types of tombstones are stored.

        By default, after 10 days, data that is marked by a tombstone is freed with a compaction execution. 
        This time can be configured and reduced using the gc_grace_seconds option in the Cassandra configuration.

        When is a compaction executed?

            When the operation is executed depends mainly on the selected strategy. 
            In general, a compaction execution takes SSTables and creates new SSTables out of it.

            The most common executions are:
                1.  When conditions for a compaction are true, that triggers compaction execution when data is inserted
                2. A manually executed major compaction using the nodetool

        Sometimes, tombstones not deleted for the following reasons:

            1. Null values mark values to be deleted and are stored as tombstones. 
            This can be avoided by either replacing null with a static value, 
            or not setting the value at all if the value is null
            
            2. Empty lists and sets are similar to null for Cassandra and create a tombstone, 
            so donâ€™t insert them if theyâ€™re empty. Take care to avoid null pointer exceptions 
            when storing and retrieving data in your application
            
            3. Updated lists and sets create tombstones. If you update an entity and 
            the list or set does not change, it still creates a tombstone to empty the list 
            and set the same values. Therefore, only update necessary fields to avoid issues. 
            The good thing is, they are compacted due to the new values

        If you have many tombstones, you might run into another Cassandra 
        issue that prevents a query from being executed.

        This happens when the tombstone_failure_threshold is reached, 
        which is set by default to 100,000 tombstones. This means that, 
        when a query has iterated over more than 100,000 tombstones, it will be aborted.

        The issue here is, once a query stops executing, 
        itâ€™s not easy to tidy things up because Cassandra will stop 
        even when you execute a delete, as it has reached the tombstone limit.

        Usually you would never have that many tombstones. 
        But mistakes happen, and you should take care to avoid this case.

        There is a handy operation metric that you should observe called 
        TombstoneScannedHistogram to avoid unexpected issues in production.

        






https://www.freecodecamp.org/news/the-apache-cassandra-beginner-tutorial/

https://madhuramehendale.medium.com/partition-key-in-cassandra-f36d8670375c