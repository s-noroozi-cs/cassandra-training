1. Enable incremental backups 
    (only needs to be done once, persisted in cassandra.yaml)
    Edit cassandra.yaml and set:
        incremental_backups: true

        sed -i 's/incremental_backups: false/incremental_backups: true/' /etc/cassandra/cassandra.yaml
    Then restart Cassandra, or run the following to enable it on the fly
        nodetool enablebackup

2. create keyspace and table

    cqlsh> CREATE KEYSPACE rayan WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
    
    cqlsh> CREATE TABLE rayan.user (id int PRIMARY KEY , note text);

    cqlsh> INSERT INTO rayan.user (id , note ) VALUES ( 1,'hi');

    cqlsh> INSERT INTO rayan.user (id , note ) VALUES ( 2,'bye');

    cqlsh> SELECT * FROM rayan.user ;

        id | note
        ---+------
        1  |   hi
        2  |  bye

3. create snapshot

    $ nodetool snapshot -t snapshot_base rayan

    $ ls -l /var/lib/cassandra/data/rayan

4. changing data

    cqlsh> INSERT INTO rayan.user (id , note ) VALUES ( 3,'why');
    cqlsh> UPDATE rayan.user SET note = 'bye bye' WHERE id =2;
    cqlsh> SELECT * FROM  rayan.user ;

        id  | note
        ----+---------
        1   |      hi
        2   | bye bye
        3   |      why

5. force to write to disk (create and update backup)

    $ nodetool flush

    # verify change date of base dir, backup and snapshot
    $ ls -l /var/lib/cassandra/data/rayan/

6. simulate disaster or any other event to lost data

    cqlsh> TRUNCATE rayan.user ;

    #verify it
    cqlsh> SELECT * FROM rayan.user

7. create directoy path with pattern {keyspace}/{table}

    $ mkdir -p /rayan/user
    
    # copy snapshot and backup directoy content to above created dir

    $ sstableloader --nodes 172.17.0.2 /rayan/user/


------------------------------------------------------------------------

=== tools for restore scenario ===

* nodetool refresh is deprecated, nodetool import is its direct replacement.

sstableloader (high-level) vs. nodetool import (low-level)

Core Function	
    
    sstableloader
        Streams data from SSTables into a live cluster.	
    
    nodetool import
        Loads new SSTables that were manually copied into the data 
        directory of a single, specific node.

Analogy
    
    sstableloader
        A Smart Postal Network: You give it a package, and it figures out 
        how to deliver it to every correct address.	

    nodetool import
        Hand-Delivery to One House: You must take the package to the 
        exact address and place it in the correct room yourself.

Cluster Topology

    sstableloader
        Topology-Aware. It connects to the cluster, 
        understands the ring (tokens, replicas), 
        and streams data to all correct nodes automatically.	

    nodetool import
        Topology-Oblivious. It only works on the single node you copied 
        the files to. It doesn't know about the wider cluster.

Required Follow-up	

    sstableloader
        None. The process is complete and consistent when it finishes.	

    nodetool import
        Must run nodetool repair. You have only loaded data on one replica. 
        You must repair the table to stream that data to the other replicas 
        and achieve consistency.

Primary Use Case

    sstableloader
        Restoring backups, migrating data between different clusters, 
        populating a new cluster.	

    nodetool import 
        Initial bulk load of pre-generated data onto a specific node, 
        or a manual cold restore to an identical cluster.

Use sstableloader if:

    * You are restoring a backup (snapshot + incrementals).

    * The target cluster is live and you cannot afford downtime.

    * The target cluster has a different number of nodes or a 
        different token allocation than the source.

    * You want the safest, most automated method.

    * This is the recommended choice for 99% of cases, 
        especially backup restoration.

Use nodetool import if:

    * You are performing a full-cluster disaster recovery onto an 
        identical cluster (same number of nodes, same tokens).

    * You are loading a very large, pre-generated dataset and want the 
        absolute fastest load time by bypassing the write path.

    * You are an expert and have a reason to need this low-level control.

    * You are okay with taking a cluster downtime for a manual operation.