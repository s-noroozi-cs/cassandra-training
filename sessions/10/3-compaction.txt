Compaction
    Compaction is Cassandra's automatic process of merging multiple 
    data files (SSTables) into a single, new file to optimize read 
    performance and reclaim space from expired or deleted data.

How Data is Written
    When you write data to Cassandra, 
    it first goes to the commit log (for durability) and then 
    to an in-memory structure called a memtable. 
    When a memtable is full, it is flushed to disk as a read-only 
    file called an SSTable (Sorted String Table).

The Problem
    Over time, you accumulate many SSTables. A piece of data might be 
    spread across multiple SSTables, the original value in one, an 
    update in another, and a tombstone marking its deletion in a third. 
    This makes reads very slow because Cassandra has to check every 
    relevant SSTable to assemble the most recent state of the data.

The Solution - Compaction

    1. Merging SSTables: 
        It takes several smaller SSTables and merges them into one 
        larger, consolidated SSTable.

    2. Combining Data: 
        During the merge, it only keeps the most recent version of 
        each row and column.

    3. Dropping Expired Data:
        It discards any data that has exceeded its TTL.

    4. Dropping Tombstones: 
        It can remove tombstones (delete markers) once the data they 
        reference is older than gc_grace_seconds.

Why Compaction is Vital:

    1. Faster Reads: 
        Fewer SSTables to check = lower read latency

    2. Space Reclamation: 
        It frees up disk space used by outdated data

    3. Data Consistency: 
        It helps ensure data is consistent and old data doesn't reappear

Potential Negative Effects & Considerations

    1. Resource Consumption (CPU, I/O, Disk): 
        Compaction is a resource-intensive operation. 
        It reads and rewrites large amounts of data. On a busy cluster, 
        this can compete with application read/write requests, 
        potentially causing higher latency during the compaction process. 
        This is the classic trade-off: short-term pain for long-term gain.

    2. Disk Space Requirement: 
        During compaction, the system needs extra disk space. It must write 
        the new, compacted SSTable before it can delete the old ones. 
        You need to have enough free disk space 
        (often recommended to be 50% of total capacity) to handle these 
        operations without running out of space.

    3. Long-Running Compactions: 
        If a node falls behind (e.g., due to being down for maintenance), 
        it might come back and have a huge backlog of SSTables to compact. 
        This can lead to a long-running "compaction storm" that takes hours or 
        even days to complete, during which performance can be significantly 
        impacted.

Common Compaction Strategies:

    SizeTieredCompactionStrategy (STCS): 
        
        Definition: 
            The default strategy. STCS groups SSTables of similar size 
            and compacts them together into a larger SSTable. This process 
            creates a "tiered" effect of increasingly larger SSTables.

        How it Works:
            1. It triggers when a certain number (default: 4) of SSTables of 
            a similar size exist.

            2. It compacts these 4 SSTables into 1 larger one.

            3. Once there are 4 of these larger SSTables, they are 
            compacted into one even larger SSTable, and so on.

        Configuration (common options in table_definition):

            'min_threshold': 4 
                The number of same-sized SSTables needed to trigger 
                a compaction.

            'max_threshold': 32
                The maximum number of SSTables to compact at once.

            'bucket_high': 1.5

            'bucket_low': 0.5

        Effect & Best For:

            Pros: 
                Simple, efficient for write-intensive workloads 
                where reads are less frequent. It minimizes total 
                write I/O during compaction.

            Cons: 
                Can lead to temporary space amplification 
                (needs up to 50% free disk space during the process). 
                Can cause higher read latency if data is spread across 
                many large SSTables. Does not handle time-series data with 
                TTL well.

            Analogy: 
                Combining many small boxes into a few larger boxes.

    LeveledCompactionStrategy (LCS): 
        Good for read-heavy workloads. Uses levels of SSTables 
        to ensure 90% of reads can be satisfied by a single SSTable.

    TimeWindowCompactionStrategy (TWCS): 
        Designed for time-series data. Compacts data within the same 
        time window, making it very efficient for TTL-based expiration.