1. start cassandra cluster with two nodes 
    
    $ docker compose up

2. check health of container using 
    
    $ docker ps 

3. check cluster status in each node separately

    $ docker exec cassandra1 nodetool status

    $ docker exec cassandra2 nodetool status

4. to simulate partitioning scenario exist different solutions

    4.1 using iptables to block traffic of communication of gossip
        this approach gives you fine-grained control over gossip communication 
        while keeping other ports (like CQL port 9042) available 
        for testing client behavior during partitions.

        This approach needs to install iptables package,
        We assume iptables is installed.

        # block gossip traffic from node 2 to others
        $ docker exec cassandra2 iptables -A OUTPUT -p tcp --dport 7000 -j DROP

        block gossip traffic from node 1 to node 2
        $ docker exec cassandra1 iptables -A OUTPUT -d cassandra2 -p tcp --dport 7000 -j DROP


        to clean up iptables using following 

        $ docker exec cassandra1 iptables -F
        $ docker exec cassandra2 iptables -F


    4.2 using simpler solution - that work on lab and best option on local machine
        using docker network disconnect 

        $ docker network disconnect network_name node_name

        $ docker network connect network_name node_name

5. using docker network to disconnect network of one node  

    5.1 find docker correct network name

        $ docker network ls
            // in our scenario , network name like this '05_cassandra-net'

    5.2 disconnect network of node 2 (cassandra2)

        $ docker network disconnect 05_cassandra-net cassandra2

    5.3 then docker compose log should be display gossip network issue to sync.

        * java.net.NoRouteToHostException
        * io.netty.channel.ConnectTimeoutException
    
    5.4 in each nodes, when you check status , current node is up and other is down

        $ docker exec cassandra1 nodetool status

        $ docker exec cassandra2 nodetool status

6. reconnect network and douple check connectivity of cluster nodes

    6.1 connect the disconnected node
    
        $ docker network connect 05_cassandra-net cassandra2

    6.2 check docker compose logs

        should be see gossip logs that display node x is up

    6.3 check node status of cassandra cluster

        $ docker exec cassandra1 nodetool status

        $ docker exec cassandra2 nodetool status

7. get cassandra query cli (cqlsh) from each node separately to simulate our scenario
    
    *** i use iterm - terminal that allow to split screen ***

    $ docker exec -it cassandra1 cqlsh

    $ docker exec -it cassandra2 cqlsh

8. create simple keyspace and table and verify them

    # execute create command on node A and verify it on node B

    cqlsh> CREATE KEYSPACE IF NOT EXISTS rayan 
        WITH REPLICATION = { 
            'class' : 'SimpleStrategy','replication_factor' : '1'
        };

    cqlsh> DESCRIBE KEYSPACES;
    cqlsh> SELECT * FROM system_schema.keyspaces;

    cqlsh> CREATE TABLE IF NOT EXISTS rayan.user (
        user_id text PRIMARY KEY,
        last_cmd text);

    cqlsh> desc rayan;
    cqlsh> desc tables;

9. insert 3 records on node A and B 

    cqlsh> insert into rayan.user(user_id,last_cmd) values('1','list');
    cqlsh> insert into rayan.user(user_id,last_cmd) values('2','move');
    cqlsh> insert into rayan.user(user_id,last_cmd) values('3','trace');

10. verify insert operation and also activating tracing to see what happan

    cqlsh> SELECT * FROM rayan.user;

    cqlsh> TRACING ON;

    cqlsh> SELECT * FROM rayan.user;

11. 


